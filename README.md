# ğŸš€ CUDA Image Filter Accelerator

High-performance CUDA application that applies advanced image filtering using multiple GPU kernel strategies. Optimized to leverage shared memory, constant memory, and arithmetic intensity for real-time edge detection on grayscale images. Kernel 5 achieves over **2Ã— speedup** compared to baseline and includes additional transfer optimizations using pinned memory.

> ğŸ§  Built as a systems-level project for GPU parallel computing (CSC367H5).
> ğŸ› ï¸ Developed and profiled 5 custom kernels with increasing levels of optimization.
> âœ… Final implementation includes custom timers, modular utility functions, and high-efficiency normalization logic.

---

## ğŸ§± Tech Stack

| Language   | Tools & Libraries | Infrastructure    |
| ---------- | ----------------- | ----------------- |
| C++ / CUDA | nvcc, pthreads    | Linux, Make/CMake |

---

## ğŸ” Project Overview

This project accelerates 2D image filtering using CUDA kernels. Each grayscale image is filtered using Laplacian kernels, followed by normalization of the output based on computed min/max values.

### Key Capabilities:

* Efficient parallel pixel computation using multiple kernel designs.
* Flexible support for 3x3, 5x5, and 9x9 Laplacian filters.
* Modular utility architecture (`kernel_common.cu`) for memory allocation, transfer, and cleanup.
* Advanced normalization kernel with smart skipping for uniform outputs.

---

## ğŸ§ª Performance Highlights

| Optimization Technique             | Impact                           |
| ---------------------------------- | -------------------------------- |
| Shared Memory Reductions           | Improved per-block min/max speed |
| Constant Memory for Filters        | Faster repeated reads            |
| Pinned Host Memory                 | 2Ã— faster transfer times         |
| Kernel Fusion (filter + reduction) | \~200% speedup vs baseline       |
| Removed Redundant Branching        | Reduced warp divergence          |
| Arithmetic Precomputation          | Cleaner and faster kernel logic  |
| Row-Major Access Optimization      | Improved spatial locality        |

> âœ… Kernel 5 outperformed all others with **>10% runtime improvement** for medium and large images
> ğŸ”„ Transfer time reduced significantly with pinned memory
> ğŸ”¬ Edge cases (uniform images) handled optimally with skipped normalization

---

Command Line Flags:

* `-i`: input image file
* `-o`: output image file
* `-f`: filter type (1: 3x3, 2: 5x5, 3: 9x9, 4: identity)
* `-m`: method (1â€“5: various kernels)
* `-n`: threads per block (e.g., 256)
* `-t`: toggle timing (1 = on, 0 = off)


### ğŸ”¨ Compile with CMake

```bash
mkdir build && cd build
cmake ../ -DCMAKE_BUILD_TYPE=Release
make
```

> âš ï¸ Use `Release` mode (`-O3`) for performance measurements.
> âœ… Outputs: `main`, `pgm_creator`, `test_solution`

---

## ğŸ§  What I Learned

* Deepened understanding of CUDA execution model and GPU memory hierarchy
* Practiced reduction strategies with intra-block shared memory coordination
* Learned to balance compute and memory access patterns for GPU efficiency
* Applied software engineering best practices to C++/CUDA:

  * Functional decomposition
  * Modular utility headers
  * RAII-style custom timers
* Gained insight into GPU-specific performance tuning and measurement tooling

---

## ğŸ§ª Testing

Run built-in tests to validate correctness:

```bash
./test_solution
```

> Add additional tests in `tests.cu` as needed to verify edge cases or new kernels.

---

## ğŸ¤ Collaboration & Credits

> ğŸ’¡ Developed independently with architectural mentoring.
> ğŸ§  Optimization insights and performance tuning guided by experimentation.

---

## ğŸ”— Links
* ğŸ“ Course: CSC367H5 â€“ Parallel Programming at University of Toronto
